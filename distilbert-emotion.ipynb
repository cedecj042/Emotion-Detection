{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing ml libraries\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn import metrics\n",
    "import transformers\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "import re\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('processed/final-train.csv')\n",
    "dev_df = pd.read_csv('processed/final-dev.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Setting up the device for GPU usage\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming_score(y_true, y_pred, normalize=True, sample_weight=None):\n",
    "    acc_list = []\n",
    "    for i in range(y_true.shape[0]):\n",
    "        set_true = set( np.where(y_true[i])[0] )\n",
    "        set_pred = set( np.where(y_pred[i])[0] )\n",
    "        tmp_a = None\n",
    "        if len(set_true) == 0 and len(set_pred) == 0:\n",
    "            tmp_a = 1\n",
    "        else:\n",
    "            tmp_a = len(set_true.intersection(set_pred))/\\\n",
    "                    float( len(set_true.union(set_pred)) )\n",
    "        acc_list.append(tmp_a)\n",
    "    return np.mean(acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>labels</th>\n",
       "      <th>average_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>worry payment problem may never joyce meyer mo...</td>\n",
       "      <td>[0, 1, 0, 0, 1, 0, 0, 1]</td>\n",
       "      <td>[0.00333792 1.         0.0042222  0.76656254 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>whatever decide make sure make happy</td>\n",
       "      <td>[0, 1, 0, 0, 1, 0, 0, 1]</td>\n",
       "      <td>[0.         0.86536213 0.11912322 0.00963587 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>help drowning thoughts</td>\n",
       "      <td>[0, 0, 0, 1, 0, 1, 0, 0]</td>\n",
       "      <td>[0.         0.39879283 0.04923599 0.82461023 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>help brother drowning</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 1, 0]</td>\n",
       "      <td>[2.26823486e-04 3.45036286e-03 0.00000000e+00 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>also help majority nfl coaching inept bill bri...</td>\n",
       "      <td>[1, 1, 1, 0, 1, 0, 0, 0]</td>\n",
       "      <td>[1.         0.00347338 0.96239843 0.00468326 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11111</th>\n",
       "      <td>feel like people life very supportive others n...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
       "      <td>[0.02367427 0.02209223 0.02753819 0.2476681  1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11112</th>\n",
       "      <td>feel loyal sen</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
       "      <td>[0.         0.00566371 0.00191819 0.81291933 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11113</th>\n",
       "      <td>feel complicit supporting owning copy</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
       "      <td>[0.         0.00597871 0.00205567 0.19240879 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11114</th>\n",
       "      <td>really feel like supporting helping</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
       "      <td>[0.00368038 0.00478825 0.00875506 0.23504549 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11115</th>\n",
       "      <td>feel worth supporting</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
       "      <td>[0.         0.00495722 0.00160853 0.19203645 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11116 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  tweets  \\\n",
       "0      worry payment problem may never joyce meyer mo...   \n",
       "1                   whatever decide make sure make happy   \n",
       "2                                 help drowning thoughts   \n",
       "3                                  help brother drowning   \n",
       "4      also help majority nfl coaching inept bill bri...   \n",
       "...                                                  ...   \n",
       "11111  feel like people life very supportive others n...   \n",
       "11112                                     feel loyal sen   \n",
       "11113              feel complicit supporting owning copy   \n",
       "11114                really feel like supporting helping   \n",
       "11115                              feel worth supporting   \n",
       "\n",
       "                         labels  \\\n",
       "0      [0, 1, 0, 0, 1, 0, 0, 1]   \n",
       "1      [0, 1, 0, 0, 1, 0, 0, 1]   \n",
       "2      [0, 0, 0, 1, 0, 1, 0, 0]   \n",
       "3      [0, 0, 0, 1, 0, 0, 1, 0]   \n",
       "4      [1, 1, 1, 0, 1, 0, 0, 0]   \n",
       "...                         ...   \n",
       "11111  [0, 0, 0, 0, 0, 0, 0, 1]   \n",
       "11112  [0, 0, 0, 0, 0, 0, 0, 1]   \n",
       "11113  [0, 0, 0, 0, 0, 0, 0, 1]   \n",
       "11114  [0, 0, 0, 0, 0, 0, 0, 1]   \n",
       "11115  [0, 0, 0, 0, 0, 0, 0, 1]   \n",
       "\n",
       "                                           average_score  \n",
       "0      [0.00333792 1.         0.0042222  0.76656254 0...  \n",
       "1      [0.         0.86536213 0.11912322 0.00963587 1...  \n",
       "2      [0.         0.39879283 0.04923599 0.82461023 0...  \n",
       "3      [2.26823486e-04 3.45036286e-03 0.00000000e+00 ...  \n",
       "4      [1.         0.00347338 0.96239843 0.00468326 0...  \n",
       "...                                                  ...  \n",
       "11111  [0.02367427 0.02209223 0.02753819 0.2476681  1...  \n",
       "11112  [0.         0.00566371 0.00191819 0.81291933 0...  \n",
       "11113  [0.         0.00597871 0.00205567 0.19240879 0...  \n",
       "11114  [0.00368038 0.00478825 0.00875506 0.23504549 0...  \n",
       "11115  [0.         0.00495722 0.00160853 0.19203645 0...  \n",
       "\n",
       "[11116 rows x 3 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.DataFrame()\n",
    "train['tweets'] = train_df['processed']\n",
    "train['labels'] = train_df.iloc[:, 1:9].values.tolist()\n",
    "train['average_score'] = train_df['average_score']\n",
    "\n",
    "dev = pd.DataFrame()\n",
    "dev['tweets'] = dev_df['processed']\n",
    "dev['labels'] = dev_df.iloc[:,1:9].values.tolist()\n",
    "dev['average_score'] = dev_df['average_score']\n",
    "\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sections of config\n",
    "# Defining some key variables that will be used later on in the training\n",
    "MAX_LEN = 64\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "VALID_BATCH_SIZE = 8\n",
    "EPOCHS = 12\n",
    "LEARNING_RATE = 1e-06\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', truncation=True, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.text = dataframe.tweets\n",
    "        self.labels = dataframe.labels\n",
    "        self.average_score = dataframe.average_score\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.text[index])\n",
    "        text = \" \".join(text.split())\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "        labels = torch.tensor(self.labels[index], dtype=torch.float)\n",
    "        \n",
    "        average_score_str = self.average_score[index].strip('[]')\n",
    "        average_score_list = [float(score.strip()) for score in average_score_str.split(' ') if score.strip()]\n",
    "        average_score = torch.tensor(average_score_list, dtype=torch.float)\n",
    "    \n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'labels': labels,\n",
    "            'average_score': average_score\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (11116, 3)\n",
      "TRAIN Dataset: (886, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ids': tensor([  101,  4737,  7909,  3291,  2089,  2196, 11830, 11527, 14354,  4105,\n",
       "          4737,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0]),\n",
       " 'mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'labels': tensor([0., 1., 0., 0., 1., 0., 0., 1.]),\n",
       " 'average_score': tensor([0.0033, 1.0000, 0.0042, 0.7666, 0.4931, 0.8112, 0.0000, 0.4740])}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(\"FULL Dataset: {}\".format(train.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(dev.shape))\n",
    "# print(\"TEST Dataset: {}\".format(test.shape))\n",
    "\n",
    "training_set = MultiLabelDataset(train, tokenizer, MAX_LEN)\n",
    "testing_set = MultiLabelDataset(dev, tokenizer, MAX_LEN)\n",
    "training_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBERTClass(\n",
       "  (l1): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (1): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (2): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (3): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (4): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (5): TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=776, out_features=8, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the customized model, by adding a drop out and a dense layer on top of distil bert to get the final output for the model. \n",
    "\n",
    "class DistilBERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DistilBERTClass, self).__init__()\n",
    "        self.l1 = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
    "        self.dropout = torch.nn.Dropout(0.1)\n",
    "        self.classifier = torch.nn.Linear(768+8,8)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, average_score):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state = output_1[0]\n",
    "        pooler = hidden_state[:, 0]\n",
    "        pooler = self.pre_classifier(pooler)\n",
    "        pooler = torch.nn.Tanh()(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "        # Concatenate tokenized inputs with average_score\n",
    "        combined = torch.cat((pooler,average_score), dim=1)\n",
    "        output = self.classifier(combined)\n",
    "        return output\n",
    "\n",
    "model = DistilBERTClass()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs, targets):\n",
    "    # print(outputs,targets)\n",
    "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for _, data in tqdm(enumerate(training_loader, 0)):\n",
    "        ids = data['ids'].to(device, dtype=torch.long)\n",
    "        mask = data['mask'].to(device, dtype=torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
    "        average_score = data['average_score'].to(device, dtype=torch.float)\n",
    "        targets = data['labels'].to(device, dtype=torch.float)\n",
    "        outputs = model(ids, mask, token_type_ids, average_score)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        if _%5000==0:\n",
    "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix, plot_confusion_matrix\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "def validation(testing_loader):\n",
    "    model.eval()\n",
    "    fin_targets = []\n",
    "    fin_outputs = []\n",
    "    with torch.no_grad():\n",
    "        for _, data in tqdm(enumerate(testing_loader, 0)):\n",
    "            ids = data['ids'].to(device, dtype=torch.long)\n",
    "            mask = data['mask'].to(device, dtype=torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
    "            average_score = data['average_score'].to(device, dtype=torch.float)  \n",
    "            targets = data['labels'].to(device, dtype=torch.float)  # Ensure this is the correct key for your labels\n",
    "\n",
    "            outputs = model(ids, mask, token_type_ids, average_score)  # Pass emotion_scores to the model\n",
    "\n",
    "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "    # Convert lists to numpy arrays for easier processing\n",
    "    fin_targets = np.array(fin_targets)\n",
    "    fin_outputs = np.array(fin_outputs)\n",
    "\n",
    "    # Convert binary outputs to class labels\n",
    "    final_outputs = np.where(fin_outputs >= 0.5, 1, 0)\n",
    "\n",
    "    # Calculate precision, recall, and F1 score\n",
    "    report = classification_report(fin_targets, final_outputs, output_dict=True)\n",
    "    precision = report['weighted avg']['precision']\n",
    "    recall = report['weighted avg']['recall']\n",
    "    f1_score = report['weighted avg']['f1-score']\n",
    "\n",
    "    # Calculate accuracy manually\n",
    "    accuracy = np.mean(fin_targets == final_outputs)\n",
    "\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1 Score: {f1_score}\")\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "    return final_outputs, fin_targets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 31.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss:  0.6974718570709229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1390it [00:49, 28.24it/s]\n",
      "4it [00:00, 29.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss:  0.4691627621650696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1390it [00:53, 26.15it/s]\n",
      "4it [00:00, 29.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Loss:  0.23330560326576233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1390it [00:51, 26.75it/s]\n",
      "6it [00:00, 28.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Loss:  0.20665127038955688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1390it [00:51, 27.02it/s]\n",
      "4it [00:00, 30.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Loss:  0.42200881242752075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1390it [00:51, 26.85it/s]\n",
      "3it [00:00, 29.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Loss:  0.17278921604156494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1390it [00:51, 26.85it/s]\n",
      "3it [00:00, 29.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Loss:  0.37243330478668213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1390it [00:51, 27.09it/s]\n",
      "6it [00:00, 28.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Loss:  0.220111683011055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1390it [00:51, 27.08it/s]\n",
      "4it [00:00, 29.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Loss:  0.28109997510910034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1390it [00:50, 27.36it/s]\n",
      "3it [00:00, 29.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Loss:  0.12249624729156494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1390it [00:50, 27.47it/s]\n",
      "4it [00:00, 29.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Loss:  0.1811279058456421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1390it [00:50, 27.44it/s]\n",
      "4it [00:00, 29.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Loss:  0.20254802703857422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1390it [00:50, 27.42it/s]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    train(epoch) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "111it [00:00, 112.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.7632003811877183\n",
      "Recall: 0.6502325581395348\n",
      "F1 Score: 0.7000984205719701\n",
      "Accuracy: 0.8325338600451467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "outputs, targets = validation(testing_loader)\n",
    "\n",
    "final_outputs = np.array(outputs) >= 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamming Score = 0.5822234762979683\n",
      "Hamming Loss = 0.16746613995485327\n"
     ]
    }
   ],
   "source": [
    "val_hamming_loss = metrics.hamming_loss(targets, final_outputs)\n",
    "val_hamming_score = hamming_score(np.array(targets), np.array(final_outputs))\n",
    "\n",
    "print(f\"Hamming Score = {val_hamming_score}\")\n",
    "print(f\"Hamming Loss = {val_hamming_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved\n"
     ]
    }
   ],
   "source": [
    "# Saving the files for inference\n",
    "\n",
    "output_model_file = 'models/distilbert_emotions.bin'\n",
    "output_vocab_file = 'models/vocab_distilbert_emotions.bin'\n",
    "\n",
    "# torch.save(model.state_dict(), output_model_file)\n",
    "torch.save(model, output_model_file)\n",
    "tokenizer.save_vocabulary(output_vocab_file)\n",
    "\n",
    "print('Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
