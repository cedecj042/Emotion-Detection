{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in /usr/local/lib/python3.9/dist-packages (2.12.1)\n",
      "Requirement already satisfied: contractions in /usr/local/lib/python3.9/dist-packages (0.1.73)\n",
      "Requirement already satisfied: typing-extensions>=4.7.0 in /usr/local/lib/python3.9/dist-packages (from emoji) (4.11.0)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.9/dist-packages (from contractions) (0.0.24)\n",
      "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.9/dist-packages (from textsearch>=0.0.21->contractions) (2.1.0)\n",
      "Requirement already satisfied: anyascii in /usr/local/lib/python3.9/dist-packages (from textsearch>=0.0.21->contractions) (0.3.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install emoji contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T18:46:12.040479Z",
     "iopub.status.busy": "2024-04-28T18:46:12.039692Z",
     "iopub.status.idle": "2024-04-28T18:46:13.254266Z",
     "shell.execute_reply": "2024-04-28T18:46:13.253189Z",
     "shell.execute_reply.started": "2024-04-28T18:46:12.040439Z"
    },
    "id": "FRQa-Jr4Hb0t"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import emoji\n",
    "import pandas as pd\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "yrGTLjn6g6sM"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSON file into a Python dictionary\n",
    "with open('data/emoticon_dict.json', 'r') as file:\n",
    "    emotion_dict = json.load(file)\n",
    "\n",
    "def replace_emoticons_with_emotions(text, emotion_dict):\n",
    "    for emoticon, emotion in emotion_dict.items():\n",
    "        text = text.replace(emoticon, emotion)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-04-28T18:29:50.537296Z",
     "iopub.status.busy": "2024-04-28T18:29:50.536905Z",
     "iopub.status.idle": "2024-04-28T18:29:50.552595Z",
     "shell.execute_reply": "2024-04-28T18:29:50.551706Z",
     "shell.execute_reply.started": "2024-04-28T18:29:50.537266Z"
    },
    "id": "huxb9TMahFao",
    "outputId": "40366e56-6330-4dc9-a0f3-1ae89f19efbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm so happy :) but I'm also frustrated :angry_face: and confused :confused_face:.\n"
     ]
    }
   ],
   "source": [
    "def convert_emojis_to_text(text):\n",
    "    return emoji.demojize(text)\n",
    "\n",
    "def contains_emoji(text):\n",
    "    # Create a regular expression for emojis\n",
    "    emoji_regex = emoji.get_emoji_regexp()\n",
    "\n",
    "    # Check if the text contains an emoji\n",
    "    if emoji_regex.search(text):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "# Example usage\n",
    "text = \"I'm so happy :) but I'm also frustrated ðŸ˜  and confused ðŸ˜•.\"\n",
    "text_with_emojis_converted = convert_emojis_to_text(text)\n",
    "print(text_with_emojis_converted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T18:29:51.489140Z",
     "iopub.status.busy": "2024-04-28T18:29:51.488134Z",
     "iopub.status.idle": "2024-04-28T18:29:51.501220Z",
     "shell.execute_reply": "2024-04-28T18:29:51.500264Z",
     "shell.execute_reply.started": "2024-04-28T18:29:51.489095Z"
    },
    "id": "ad5FznayhPhk"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "\n",
    "def create_dict_from_csv(file_name):\n",
    "    dictionary = {}\n",
    "    with open(file_name, mode='r') as file:\n",
    "        csv_reader = csv.reader(file)\n",
    "        next(csv_reader) # Skip the header row\n",
    "        for row in csv_reader:\n",
    "            key = row[0]\n",
    "            value = row[1]\n",
    "            dictionary[key] = value\n",
    "    return dictionary\n",
    "\n",
    "abbreviations_dict = create_dict_from_csv('data/abbv.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T18:29:52.253534Z",
     "iopub.status.busy": "2024-04-28T18:29:52.252779Z",
     "iopub.status.idle": "2024-04-28T18:29:52.258392Z",
     "shell.execute_reply": "2024-04-28T18:29:52.257408Z",
     "shell.execute_reply.started": "2024-04-28T18:29:52.253500Z"
    },
    "id": "PbAndImLhR3c"
   },
   "outputs": [],
   "source": [
    "def expand_abbreviations(text):\n",
    "    for abbr, full_form in abbreviations_dict.items():\n",
    "        # Use regex to match the abbreviation as a whole word to avoid partial matches\n",
    "        text = re.sub(r'\\b' + abbr + r'\\b', full_form, text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T18:29:53.002412Z",
     "iopub.status.busy": "2024-04-28T18:29:53.002044Z",
     "iopub.status.idle": "2024-04-28T18:29:53.028307Z",
     "shell.execute_reply": "2024-04-28T18:29:53.027353Z",
     "shell.execute_reply.started": "2024-04-28T18:29:53.002386Z"
    },
    "id": "YSivWUdDhXVm"
   },
   "outputs": [],
   "source": [
    "def custom_correct_spelling(text, correct_words):\n",
    "    for word in correct_words:\n",
    "        if word in text:\n",
    "            text = text.replace(word, correct_words[word])\n",
    "    return text\n",
    "\n",
    "correct_words = {\"hapy\": \"happy\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T18:29:53.701200Z",
     "iopub.status.busy": "2024-04-28T18:29:53.700808Z",
     "iopub.status.idle": "2024-04-28T18:29:53.707387Z",
     "shell.execute_reply": "2024-04-28T18:29:53.706175Z",
     "shell.execute_reply.started": "2024-04-28T18:29:53.701174Z"
    },
    "id": "SFWWGdcBhYfZ"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_repeated_chars(s):\n",
    "    # Match a character followed by at least 2 repetitions and replace with just two repetitions\n",
    "    return re.sub(r'(\\w)\\1{2,}', r'\\1\\1', s)\n",
    "\n",
    "def remove_spaces(text):\n",
    "    # Regex pattern to find spaces between letters\n",
    "    pattern = r\"(?<=\\b\\w) (?=\\w\\b)\"\n",
    "    # Using re.sub to replace the matched spaces with an empty string\n",
    "    text = re.sub(pattern, '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hyphenated_capitalized_words(text):\n",
    "    pattern = r'\\b([A-Z][a-z]*-)*([A-Z][a-z]*)\\b'\n",
    "    result = re.sub(pattern, lambda match: match.group(0).replace('-', ''), text)\n",
    "    return result\n",
    "\n",
    "def normalize_hashtags(hashtags):\n",
    "    processed_hashtags = []\n",
    "    for hashtag in hashtags:\n",
    "        # Insert spaces before each capital letter\n",
    "        processed_hashtag = re.sub(r'([A-Z])', r' \\1', hashtag)\n",
    "        processed_hashtags.append(processed_hashtag)\n",
    "    return processed_hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T18:29:55.179158Z",
     "iopub.status.busy": "2024-04-28T18:29:55.178780Z",
     "iopub.status.idle": "2024-04-28T18:29:55.187844Z",
     "shell.execute_reply": "2024-04-28T18:29:55.186338Z",
     "shell.execute_reply.started": "2024-04-28T18:29:55.179130Z"
    },
    "id": "AQO68XyahZuB"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \n",
    "    # Remove hashtags and seperate each word\n",
    "    hashtag_pattern = r'#(\\w+)'\n",
    "    hashtags = re.findall(hashtag_pattern, text)\n",
    "    processed_hashtags = normalize_hashtags(hashtags)\n",
    "    for i, hashtag in enumerate(hashtags):\n",
    "        text = text.replace(hashtag, processed_hashtags[i])\n",
    "        \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www.\\S+|\\S+\\.ly|\\S+\\.ph|\\S+\\.net', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove mentions\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    \n",
    "    # Remove newline characters\n",
    "    text = text.replace(r\"\\n\", \" \")\n",
    "\n",
    "    # Replace &amp; to and characters\n",
    "    text = text.replace(r\"&amp;\", \"and\")\n",
    "    \n",
    "    text = re.sub(r'\\.', ' ', text)    \n",
    "    \n",
    "    #remove hyphenated\n",
    "    text = remove_hyphenated_capitalized_words(text)\n",
    "\n",
    "    #Remove all caps\n",
    "    text = text.lower()\n",
    "\n",
    "    # Expand Abbrevations\n",
    "    text = expand_abbreviations(text)\n",
    "\n",
    "    # Remove Spaces between letters like e v e r y t h i n g\n",
    "    text = remove_spaces(text)\n",
    "        \n",
    "    # Fix contractions from don't to do not\n",
    "    text = contractions.fix(text)\n",
    "\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'\\/', ' or ', text)\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "\n",
    "    # # Remove repeated characters like happppyyyyy to hapy\n",
    "    text= remove_repeated_chars(text)\n",
    "\n",
    "    # # Corrects the word to its correct spelling to happy\n",
    "    text = custom_correct_spelling(text, correct_words)\n",
    "\n",
    "    # Replace nan to \"\" characters\n",
    "    text = text.replace(r\"nan\", \"\")\n",
    "    \n",
    "    # Convert emojis to textual representation\n",
    "    if contains_emoji:\n",
    "        text = convert_emojis_to_text(text)\n",
    "        \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T18:29:56.077642Z",
     "iopub.status.busy": "2024-04-28T18:29:56.076774Z",
     "iopub.status.idle": "2024-04-28T18:29:56.474860Z",
     "shell.execute_reply": "2024-04-28T18:29:56.473778Z",
     "shell.execute_reply.started": "2024-04-28T18:29:56.077606Z"
    },
    "id": "84JDACEjhaox"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stopwords_from_csv(file_path):\n",
    "    stopwords_list = pd.read_csv(file_path)['word'].tolist()\n",
    "    return stopwords_list\n",
    "\n",
    "def customize_stopwords(stopwords):\n",
    "    # Load the custom stopwords from CSV files\n",
    "    interjections = load_stopwords_from_csv('algo/interjections.csv')\n",
    "    negations = load_stopwords_from_csv('algo/negations.csv')\n",
    "    amplifiers = load_stopwords_from_csv('algo/amplifiers.csv')\n",
    "\n",
    "    # Combine the custom stopwords into a single list\n",
    "    custom_stopwords = interjections + negations + amplifiers\n",
    "    # Remove the custom stopwords from the NLTK stopwords list\n",
    "    filtered_stopwords = [word for word in stopwords if word not in custom_stopwords]\n",
    "\n",
    "    return filtered_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-28T18:29:57.271871Z",
     "iopub.status.busy": "2024-04-28T18:29:57.270438Z",
     "iopub.status.idle": "2024-04-28T18:29:57.278201Z",
     "shell.execute_reply": "2024-04-28T18:29:57.276870Z",
     "shell.execute_reply.started": "2024-04-28T18:29:57.271824Z"
    },
    "id": "G6QN_E5Xhon7"
   },
   "outputs": [],
   "source": [
    "# Initialize the DistilBert tokenizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "emotionlex_df = pd.read_csv('algo/emolex_words.csv')\n",
    "\n",
    "def tokenize(text):\n",
    "    # Tokenize the text into words\n",
    "    word_tokens = word_tokenize(text)\n",
    "\n",
    "    nltk_stopwords = stopwords.words('english')\n",
    "    customized_stopwords = customize_stopwords(nltk_stopwords)\n",
    "\n",
    "    filtered_tokens = [token for token in word_tokens if token not in customized_stopwords]\n",
    "\n",
    "    lemmatized_tokens = []\n",
    "    for word in filtered_tokens:\n",
    "        if word in emotionlex_df['word'].tolist():  # Check if word exists in NRC emotion lexicon (assuming 'word' is the column name)\n",
    "            lemmatized_tokens.append(word)  # Don't lemmatize, keep the original word\n",
    "        else:\n",
    "            lemmatized_tokens.append(lemmatizer.lemmatize(word))  # Lemmatize other words\n",
    "\n",
    "    return lemmatized_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0kjLi5O-hzHd"
   },
   "source": [
    "Applying Preprocessing and Tokenizing to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "execution": {
     "iopub.execute_input": "2024-04-28T18:30:03.632000Z",
     "iopub.status.busy": "2024-04-28T18:30:03.631581Z",
     "iopub.status.idle": "2024-04-28T18:30:25.504489Z",
     "shell.execute_reply": "2024-04-28T18:30:25.503075Z",
     "shell.execute_reply.started": "2024-04-28T18:30:03.631968Z"
    },
    "id": "s2kw2xOkhwO0",
    "outputId": "5f56318d-bcb5-41d0-af4c-63c7eda3211a"
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('dataset/tweets-train.csv')\n",
    "df['processed'] = df['Tweet'].apply(preprocess_text)\n",
    "df['processed'] = df['processed'].apply(tokenize)\n",
    "df['processed']=df['processed'].apply(lambda x: ' '.join(x).replace('\\\\n',''))\n",
    "df.drop(columns=['Tweet'], inplace=True)\n",
    "\n",
    "# Get the list of all column names\n",
    "all_columns = df.columns.tolist()\n",
    "# Move 'Processed' to the first position\n",
    "all_columns.insert(0, all_columns.pop(all_columns.index('processed')))\n",
    "# Reorder the columns\n",
    "df = df.reindex(columns=all_columns)\n",
    "\n",
    "df.to_csv('processed/clean-train.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('dataset/tweets-dev.csv')\n",
    "df['processed'] = df['Tweet'].apply(preprocess_text)\n",
    "df['processed'] = df['processed'].apply(tokenize)\n",
    "df['processed']=df['processed'].apply(lambda x: ' '.join(x).replace('\\\\n',''))\n",
    "df.drop(columns=['Tweet'], inplace=True)\n",
    "\n",
    "# Get the list of all column names\n",
    "all_columns = df.columns.tolist()\n",
    "# Move 'Processed' to the first position\n",
    "all_columns.insert(0, all_columns.pop(all_columns.index('processed')))\n",
    "# Reorder the columns\n",
    "df = df.reindex(columns=all_columns)\n",
    "\n",
    "df.to_csv('processed/clean-dev.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('dataset/tweets-test.csv')\n",
    "df['processed'] = df['Tweet'].apply(preprocess_text)\n",
    "df['processed'] = df['processed'].apply(tokenize)\n",
    "df['processed'] = df['processed'].apply(lambda x: remove_words(x, words_to_remove))\n",
    "df['processed']=df['processed'].apply(lambda x: ' '.join(x).replace('\\\\n',''))\n",
    "df.drop(columns=['Tweet'], inplace=True)\n",
    "\n",
    "# Get the list of all column names\n",
    "all_columns = df.columns.tolist()\n",
    "# Move 'Processed' to the first position\n",
    "all_columns.insert(0, all_columns.pop(all_columns.index('processed')))\n",
    "# Reorder the columns\n",
    "df = df.reindex(columns=all_columns)\n",
    "\n",
    "\n",
    "df.to_csv('processed/clean-test.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
